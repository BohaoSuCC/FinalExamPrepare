---
title: "Guidance"
author: "BohaoSu"
date: "2023-12-13"
output: 
  pdf_document:
    latex_engine: xelatex
    citation_package: natbib
    toc: TRUE
    number_sections: TRUE
    toc_depth: 2
geometry: "top=25mm, left=30mm, right=30mm, bottom=25mm, heightrounded"
linkcolor: blue
highlight-style: github
bibliography: "reference.bib"
csl: "ucl-institute-of-education-harvard.csl"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE,
                      warning = interactive(), 
                      message = interactive(), 
                      error = TRUE)
```

# Initial project scope

```{r Packages_Loading,collapse = TRUE,echo=TRUE,eval = TRUE}
library(broom)
library(car)
library(classInt)
library(corrplot)
library(crosstalk)
library(DiagrammeR)
library(dplyr)
library(fs)
library(geojsonio)
library(ggplot2)
library(ggmap)
library(here)
library(janitor)
library(maptools)
library(mapview)
library(OpenStreetMap)
library(patchwork)
library(plotly)
library(RColorBrewer)
library(readr)
library(rJava)
library(rgdal)
library(RSQLite)
library(rgeos)
library(sf)
library(sp)
library(spatstat)
library(spdep)
library(stringr)
library(tidyverse)
library(tmap)
library(tmaptools)
```

## Research Question:

-   What are the factors that might lead to xxxxxxxxxxxxx scores in the xxxxxxxx city?

## Hypothesis:

-   Null hypothesis: There is complete spatial randomness. No statistical significance exists in a set of given observations. There is no pattern - i.e. complete spatial randomness - in our data. There is no relationship between exam scores and other observed variables across London.
-   Alternative hypothesis: Our data does exhibit a pattern.

## Methodology:

1.  The first step is always cleaning and pre-processing data, which is the foundation for any kinds of analysis and modelling.

2.  Exploration Data Analysis(histograms and Q-Q plots for statistical information, KDE for spatial distribution, DBSCAN for spatial clustering, etc.) need to be done both for non-spatial and spatial fields. This step would clarify the simple relationship and some features inside the data.

3.  Based on research purpose, the regression model also needs two important prerequisite to guarantee its adaptability and rationality.

    -   The first one is "The xxxxxx's happening does have summarizable and discernible spatial distribution characteristics and spatial patterns." This indicates whether a spatial analysis rather than purely quantitative analysis should be utilized to address the research question. Hence, Spatial patterns analysis like KDE or DBSCAN should be operated to check whether there is random occurrences for the xxxxxxxxxxx or not. If the result is complete random distribution, I'll just do the basic quantitative analysis based on the non-spatial data.
    -   The second one is "Spatial location information does play as a crucial and indispensable variable when building regression models." This means which regression model should be utilized to analysis and predict the xxxxxxx. I suppose spatial autocorrelation methods should be used to examine the adaptability of Tobler's Law.[@tobler_computer_1970] If there is no evidence showing geographical elements does affect the dependant variables distribution, then linear regression model or polynomial regression model should be the options. Otherwise, we should consider spatial information and select spatial regression models like spatial lag and spatial error models or geographically weighted regression models.

4.  Afterwards, some advanced filtering or merging should be operated based on the ESDA, after which some cleansed columns and features could be extracted from the raw data and regarded as the independent variables for regression model to test the hypothesis. The variables selection process should also take some background context and research purpose into consideration.

5.  Then, modelling part should be emphasized on which model should be selected. Regression Model selection will refer to all above previous analysis and prerequisites. After establishing a baseline model, the focus shifts to evaluating and refining this model. This involves comparing the baseline model's performance against the spatial models, using metrics such as R-squared, AIC, or RMSE for validation and visualization. This process of model selection and refinement is central to achieving reliable and meaningful insights from the spatial analysis.

6.  At the End, all results and features would be generalized and summarized, and a primary research conclusion will be drawn towards the initial question.

## Potential Limitation of data and methods

### Data Limitation

-   Issues with Spatial Scale: The spatial scale of the data (e.g., geographic extent and resolution) can affect the analysis outcomes. Different spatial scales may reveal different patterns and relationships.

-   Data Quality and Completeness: The dataset may have missing or inaccurate data, leading to biased analysis results. And we will also drop part of the data due to some NAs or administrative boundaries, which will also impact on dataset's completeness.

### Methods Limitation

-   Non-independence of Spatial Data: Traditional non-spatial regression models often assume independence between observations, which may not hold true for spatial data. Spatial dependence between neighboring locations can impact the accuracy of the model. 

-   Spatial Autocorrelation in Residuals: In many regression models, spatial autocorrelation in residuals is considered a serious violation. If residuals from the model show spatial clustering, it might indicate that key variables are missing from the model.

## RMD environment configuration

Before the specific illustration and analysis procedure, some environment configuration should be down to guarantee this .RMD file's robust on various platform and devices.

-   Download .bib and .csl file remotely for reference

```{r Download_bib_csl, echo=TRUE, results='markup'}

# download reference.bib remotely from my github
download.file("https://github.com/xxxxxxx.bib", 
              destfile=here::here("reference.bib"))

# download reference.bib remotely from my github
download.file("https://raw.githubusercontent.com/BohaoSuCC/CASA0005BohaoSu/main/ucl-institute-of-education-harvard.csl", 
              destfile=here::here("ucl-institute-of-education-harvard.csl"))

```

-   Create Data Folder for Loading and Saving Data

```{r Directory_Folder}
# create the folder storing data for a better robust
folder_name <- "Data"

# get the root dir
root_dir <- here::here()

# construct the full path
folder_path <- file.path(root_dir, folder_name)

# check if the folder already exists
if (!dir.exists(folder_path)) {
  dir.create(folder_path)
  message("Folder '", folder_name, "' created at ", folder_path)
} else {
  message("Folder '", folder_name, "' already exists at ", folder_path)
}

```

# Data Introduction

## Downloading, Unzipping and loading the data

```{r Dataset_DownLoading, warning=FALSE, message=FALSE}

#Downloading the relating files and save and unzip it.
download.file("https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip", 
              destfile=here::here("Data","statistical-gis-boundaries-london.zip"))

```

```{r Dataset_Unzipping, warning=FALSE, message=FALSE}

listfiles<-dir_info(here::here("Data")) %>%
  dplyr::filter(str_detect(path, "london.zip")) %>%
  dplyr::select(path)%>%
  pull()%>%
  #print out the .gz file
  print()%>%
  as.character()%>%
  utils::unzip(exdir=here::here("Data"))

# reading the shp
Londonwards<-fs::dir_info(here::here("Data", 
                                 "statistical-gis-boundaries-london", 
                                 "ESRI"))%>%
  #$ means exact match
  dplyr::filter(str_detect(path, 
                           "London_Ward_CityMerged.shp$"))%>%
  dplyr::select(path)%>%
  dplyr::pull()%>%
  #read in the file in
  sf::st_read()

```

```{r Dataset_Reading, warning=FALSE, message=FALSE, eval = TRUE}

#Reading the csv and Add na argument to make sure csv's robust
# replace all the nas as " "
data_test <- read_csv(here::here("Data","Evictions_20231212.csv"), na=c(" ")) 

```

```{r}

LondonWardProfiles <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/772d2d64-e8c6-46cb-86f9-e52b4c7851bc/ward-profiles-excel-version.csv", 
                               col_names = TRUE, 
                               locale = locale(encoding = 'UTF-8'))


#Reading the shp file
community_areas <- st_read(here::here("Data","geo_export_7fdf694c-62dd-4de4-8f17-0b5ca2408993.shp"))
```

## Data Description

-   The dataset is mainly about xxxxxxxxxxxx, containing xxxxxxxxxxxxxxx in New York city. It is collected by xxxxxx and xxxx through xxxxx and published in the [xxxx's website](https://www.openai.com/).
-   Another data is xxxxx.shp, containing geographical information features about xxxxxxxx in xxxx city, which is published by xxxxx and can be public accessed through [xxxx's website](https://www.openai.com/).

## NA values

In the dataset, the NA values could probably mean the missed data, unrecorded observations, inapplicable data points, etc.

```{r check_NA}
#check all of the columns have been read in correctly
Column_type_list <- evictions_points %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")
total_rows <- nrow(evictions_points)

#get the na values proportion of each column
Column_NA_ratio_list <- evictions_points %>% 
  summarise_all(~sum(is.na(.))/total_rows) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_NA_ratio")

# check the CRS and any error within spatial data
st_geometry(BoroughMap)

Column_type_list
Column_NA_count_list
```

From the statistical chart we could see there are totally xxxx rows(observations) containing NAs values. Technically, I don't think it is a high rate and these NA values could have a significant impact on my analysis.

Also, I am going to consider how to deal with those NA values with different solutions according to each column's role during my analysis. Anyway, the specific solutions to these NA values should align with research question and analysis requirement, so the detailed Data processing would be demonstrated in Data Cleaning and Processing.


## Accuracy and Biasing

-   Due to the absence of some accuracy information such as measurement errors, data validation processes, etc, I will focus on the biases of the data. According to the description on the website [xxxx's website](https://www.openai.com/), the purpose of collecting these data is mainly to xxxxxxxxxxxxxxxxxxxx, which might bring about the biases of not xxxxxxxxxxxxxxxxxxxxxxxxx. However, I do not think this kind of biases will bring obvious and significant impact on analysis results and conclusions, even though the data collection methods do have limitation which I would elaborate detailedly afterwards.

## Coordinate Reference System (CRS)

-   Explain the coordinate reference system used in the data, including its type (such as geographic or projected coordinate system) and specific name (like WGS 84, UTM, etc.).

```{r}
# transform the non-spatial data into spatial data based on columns'Longitude''Latitude'
Airbnb <- read_csv("prac5_data/listings.csv") %>%
  st_as_sf(., coords = c("longitude", "latitude"), 
                   crs = 4326) %>%
    st_transform(., 27700)%>%
    # After, do some relavant filter for the useful info
    filter(room_type == 'Entire home/apt' & availability_365 =='365')

# Transform the CRS
sf_DATA_transformed <- st_transform(sf_DATA, crs = 32650)
```

-   In this analysis, we have selected the [specify CRS, e.g., WGS 84, EPSG:4326] as our Coordinate Reference System (CRS). This CRS aligns well with our study's geographic scope that includes [mention the geographical extent, e.g., multiple countries, global analysis, etc.].

-   Moreover, the impact of using [specify CRS] on my spatial analysis, especially in GWR where a spatial weight matrix really matters, is significant. And that requires distance measurement should be calculated, demonstrated and visualized precisely. Using projected CRS, I believe, should be a better choice for visualization, especially for some local-scale analysis and maps.

# Data Cleaning and Processing 

## Dealing with NAs in spatial and non-spatial dataset

Some columns, such as xxxxxxxxxx, are extremely important that we couldn't extract any useful information if there are NA values. Besides, its high accuracy makes it harder to fill missing values, which leads us to nothing else but to drop them. Some of the columns, such like xxxx and some categorical data, we also could classify all the NA values as a new category. Some columns like xxxxxxxxxx, we could assume, based on the context of the study and common sense, that the missing values are 0. Although this approach may introduce some degree of inaccuracy, it is considered a practical solution since the proportion of NA values in these columns is very high. Therefore, dropping these columns outright would be an unwise decision.

```{r Deal_NA_nonspatial, results='markup'}
# na.omit(): 从数据中删除含有 NA 的行。
# 参数：object（数据对象）。
# 默认参数：无默认参数。
library(dplyr)
DATA_cleaned <- na.omit(DATA)

# dplyr::filter(): 删除特定条件下的行。
# 参数：.DATA（数据框），...（条件表达式）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  dplyr::filter(!is.na(COLUMN_name))

# tidyr::replace_na(): 用特定值替换 NA。
# 参数：DATA（数据框），replace（用于替换的值）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  replace_na(list(COLUMN = replacement_value))

```

```{r Deal_NA_spatial}
# na.omit(): 从数据中删除含有 NA 的行。
# 参数：object（数据对象）。
# 默认参数：无默认参数。
library(dplyr)
DATA_cleaned <- na.omit(DATA)

# dplyr::filter(): 删除特定条件下的行。
# 参数：.DATA（数据框），...（条件表达式）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  dplyr::filter(!is.na(COLUMN_name))

# tidyr::replace_na(): 用特定值替换 NA。
# 参数：DATA（数据框），replace（用于替换的值）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  replace_na(list(COLUMN = replacement_value))

```

## Converting Datatype

And we will also convert some columns into specific datatype for more covenient processing.

```{r}
# as.numeric(): 将数据转换为数值类型。
# 参数：x（输入数据）。
# 默认参数：无默认参数。
DATA$COLUMN <- as.numeric(DATA$COLUMN)

#as.character(): 将数据转换为字符类型。
#参数：x（输入数据）。
#默认参数：无默认参数。
DATA$COLUMN <- as.character(DATA$COLUMN)

#as.Date(): 将数据转换为日期类型。
#参数：x（输入数据），format（日期格式）。
#默认参数：format = "%Y-%m-%d"。
DATA$date_COLUMN <- as.Date(DATA$date_COLUMN, format = "%Y-%m-%d")

DATA <- DATA %>%
  mutate(COLUMN = str_replace_all(COLUMN, "\\$", "")) %>% # remove dollar sign
  mutate(COLUMN = str_replace_all(COLUMN, ",", "")) %>%  # remove the comma
  mutate(COLUMN = as.numeric(COLUMN))  

```

## Delete or Filter outliers

```{r}
#select all spatial feature with the city boundary and transorm its CRS
BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))%>%
  st_transform(., 27700)

#delete specific rows by filter and some arguments

DATA_cleaned <- DATA %>% 
  dplyr::filter(COLUMN >= lower_limit, COLUMN <= upper_limit)

# only remain points which inside the boundary
BluePlaquesSub <- BluePlaques[BoroughMap, , op=st_within]
# to identify points completely within the borough outline, or a variety of other options such as st_overlaps, st_touches, st_contains, st_disjoint

# plot the map to check to see that they've been removed
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")

```

## Data Format Normalization 数据格式标准化

```{r}
#tolower(): 将文本转换为小写。
#参数：x（输入文本）。
#默认参数：无默认参数。
DATA$column <- tolower(DATA$column)

#toupper(): 将文本转换为大写。
#参数：x（输入文本）。
#默认参数：无默认参数。
DATA$column <- toupper(DATA$column)

#str_trim(): 去除文本两侧的空格（stringr 包）。
#参数：string（输入文本），side（修剪的方向）。
#默认参数：side = "both"。
DATA$column <- str_trim(DATA$column)

```

## Dealing with Repetitive or Unique rows

```{r}

# make sure there is no more repetitive rows in the data
BluePlaques <- distinct(BluePlaques)

#duplicated(): 检测重复行。
#参数：x（输入数据）。
#默认参数：无默认参数。
DATA_cleaned <- DATA[!duplicated(DATA), ]

#unique(): 获取唯一行。
#参数：x（输入数据）。
#默认参数：无默认参数。
DATA_cleaned <- unique(DATA)

```

## DATA Integration 数据整合

```{r}

# 两个non-spatial数据之间，还有inner_join,right_join,full_join
DATA_combined <- left_join(DATA1, DATA2, by = c('SAME_COLUMN_NAME'='SAME_COLUMN_NAME'))

# spatial data join。 Argument could be 
result <- st_join(x, y, op = st_intersects)

```

# Exploration Spatial Data Analysis (ESDA)

Exploration Spatial Data Analysis (ESDA) plays an significant role in spatial regression modeling, primarily focusing on three key aspects. 
-   First part contains acquiring statistical characteristics and distribution patterns of all non-spatial data, providing a solid foundation for selecting relevant independent variables for the model. This step is crucial for understanding the underlying structure and relationships within the data. 
-   Second part involves cluster analysis to ascertain if the data exhibits spatial clustering, indicating non-random distribution across the space. This analysis verifies one of the prerequisites for spatial regression modeling, ensuring the data's suitability for such analysis. 
-   Another essential prerequisite will be checked in part3, which is the impact of geographical spatial differences on certain dependent variables in spatial data. This is accomplished by conducting spatial autocorrelation analysis, which helps to confirm if spatial factors significantly influence the variables in question, thereby validating the use of spatial regression techniques.

## Distribution and coorelationship
```{r, echo=TRUE,eval=TRUE, warning=FALSE}
data_test$Latitude <- as.numeric(data_test$Latitude)

clean_data <- data_test %>%
  filter_all(all_vars(!is.na(.)))
```

```{r,}
ggplot(clean_data, aes(x = Latitude)) + 
  geom_histogram(binwidth = 0.01, fill = "blue", color = "black", alpha=0.5)

```

```{r, results='hold'}
# 绘制var1的直方图
ggplot(data_test, aes(x = Latitude)) + 
  geom_histogram(binwidth = 1, fill = "blue", color = "black")

# 绘制var1和var2之间的散点图
ggplot(data, aes(x = var1, y = var2)) + 
  geom_point()

# 箱子图-假设你的数据框架是data，列名为num_var
ggplot(data, aes(y = num_var)) + 
    geom_boxplot(fill = "lightblue", color = "blue")

# 条形图（Bar charts）条形图用于显示分类变量的频率。
ggplot(data, aes(x = cat_var)) + 
    geom_bar(fill = "lightgreen", color = "darkgreen")

# 计算cormatrix----假设你的数据框架是data，选择其中的几个数值型列
numeric_data <- data[c("num_var1", "num_var2", "num_var3")]
# 计算相关系数矩阵
cor_matrix <- cor(numeric_data)
# 绘制相关系数矩阵
corrplot(cor_matrix, method = "circle")
#这段代码首先计算了numeric_data中数值型变量的相关系数矩阵，
#然后使用corrplot()函数绘制出相关系数矩阵。
#method = "circle"表示使用圆圈的方式来表示相关系数的大小和方向。
#"circle"：使用圆圈，圆圈越大，相关性越强；颜色通常用来表示正负相关。
#"square"：与 "circle" 类似，但使用正方形来表示相关系数。
#"ellipse"：使用椭圆形状，椭圆的形状和方向表示相关性的强度和方向。
#"number"：直接在每个格子中显示相关系数的数值。
#"shade"：通过阴影深浅来表示相关系数的大小，通常不显示相关系数的具体数值。
#"color"：仅通过颜色来表示相关系数的大小和方向，类似于热图。
#"pie"：使用饼图来表示相关系数，饼图的大小和填充比例反映相关性的强度和方向。
```

## Several Histograms 多个图合并
```{r, echo=TRUE,eval=TRUE, warning=FALSE}
names(data_test)
```

```{r, echo=TRUE,eval=TRUE, warning=FALSE}

columns_name_datatest <- names(data_test)[c(13,14,15,16)]

# create a new list to save every histogram
plots_list <- list()

#ggplot(clean_data, aes(x = Latitude)) + 
  #geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha=0.5)

# create a histogram towards every column and add it into the plot list
for (i in 1:4) {
  column_name <- columns_name_datatest[i]
  pic <- ggplot(data_test, aes(x = !!sym(column_name))) + 
    geom_histogram(fill = "lightblue", color = "black") +
    ggtitle(paste("Histogram of", column_name)) +   #每个图的图名
    xlab(paste("X-axis of", column_name)) +  # 设置X轴标题
    ylab("Frequency") +  # 设置Y轴标题为“频率”
    theme(
      plot.title = element_text(size = 10),  # 设置图表标题字体大小
      axis.title.x = element_text(size = 8),  # 设置X轴标题字体大小
      axis.title.y = element_text(size = 8)   # 设置Y轴标题字体大小
    )
  plots_list[[i]] <- pic
}

# using patchwork to combine every histograms into a 2*2 grids
combined_plot <- wrap_plots(plots_list, ncol = 2) & 
  theme(plot.title = element_text(size = 10),
    axis.title.x = element_text(size = 8),
    axis.title.y = element_text(size = 8)
  )

combined_plot
```

## Spatial Distribution 查看空间分布

In order to understand the spatial distribution, I utilized Kernel Density Estimation (KDE) for its analysis. This method could provide with an in-depth examination of \_\_\_\_\_\_\_\_\_\_ (specific aspect or feature of the data), enabling a clearer visualization of spatial concentration and patterns within the study area. The KDE plot is particularly effective in highlighting areas of high density or clustering, which is essential for the analysis of \_\_\_\_\_\_\_\_\_\_ (specific phenomena or geographic feature).

```{r Spatial_Scatter_plot}

ggplot(DATAFRAME, aes(x = longitude, y = latitude)) + 
  geom_point() + 
  theme_minimal() + 
  labs(x = "Longitude", y = "Latitude", title = "Spatial Scatter Plot")
```

```{r Spatial_Heatmaps}
ggplot(df, aes(x = longitude, y = latitude)) + 
  stat_density_2d(aes(fill = ..level..), geom = "polygon") + 
  scale_fill_viridis_c() + 
  labs(x = "Longitude", y = "Latitude", title = "Spatial Heatmap") +
  theme_minimal()
```

-   From the Heatmaps plot we could see xxxxxxxxxxxxxxxxxxxxxxxxxxx

```{r KDE}
ggplot(df, aes(x = longitude, y = latitude)) + 
  stat_density_2d(aes(fill = ..level..), geom = "polygon") + 
  scale_fill_viridis_c() + 
  labs(x = "Longitude", y = "Latitude", title = "Spatial Heatmap") +
  theme_minimal()
```

-   From the KDE (Kernel Density Estimation) plot, we can observe \_\_\_\_\_\_\_\_\_\_. This suggests that the distribution of data exhibits a trend of \_\_\_\_\_\_\_\_\_\_, which is crucial for understanding \_\_\_\_\_\_\_\_\_\_ (specific data characteristic or geographic feature). Particularly in the region of \_\_\_\_\_\_\_\_\_\_, this distribution provides key insights into \_\_\_\_\_\_\_\_\_\_ (a relevant phenomenon or issue)."

## Spatial Patterns 查看空间模式（如聚集、离散）

```{r}

#create a sp object
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')

#create a ppp object
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)

#first extract the points from the spatial points data frame
BluePlaquesSubPoints <- BluePlaquesSub %>%
  coordinates(.)%>%
  as.data.frame()

#now run the DBSCAN analysis
db <- BluePlaquesSubPoints %>%
  fpc::dbscan(.,eps = 700, MinPts = 4)

#now plot the results
plot(db, BluePlaquesSubPoints, main = "DBSCAN Output", frame = F)
plot(BoroughMap$geometry, add=T)

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)

```

## Spatial Autocorrelation 空间自相关性

```{r}


```

# Variables Selection 特征选择与工程

## Selecting Independent Variables based on ESDA and Research Question

统计方法：例如使用相关系数或卡方检验来识别与目标变量最相关的特征。 模型基方法：如使用随机森林或LASSO回归进行特征重要性评估。 递归特征消除：这是一种利用模型精度来选择特征的方法。 特征工程：转换和创建新特征来改善模型的性能。常见的做法包括：

## DATA Normalization and Standardlization 数据规范化与标准化

```{r}
# 数据规范化
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

DATA_normalized <- as.DATA.frame(lapply(DATA, normalize))

#   2. 标准化（Standardization）
#标准化指的是将数据转换为均值为0，标准差为1的分布。这通过从每个观测中减去均值并除以标准差来实现
DATA_standardized <- scale(DATA)
#scale(x, center = TRUE, scale = TRUE)
#x 是要进行标准化的数据。
#center = TRUE 表示数据会先减去它的均值。
#scale = TRUE 表示数据会除以它的标准差。

#caret 包提供了更多高级的预处理功能。
#使用 preProcess 函数进行规范化。
library(caret)

preProcValues <- preProcess(DATA, method = c("range"))
preProcValues <- preProcess(DATA, method = c("center", "scale"))
DATA_normalized <- predict(preProcValues, DATA)
#preProcess(x, method)
#x 是要处理的数据。
#method = c("range") 指定使用范围规范化。
#method = c("center", "scale") 指定使用数据中心化和标准化。

library(dplyr)
library(purrr)

DATA_normalized <- DATA %>%
  mutate_if(is.numeric, normalize)

DATA_standardized <- DATA %>%
  mutate_if(is.numeric, scale)

```

## Create Variables for Generalising several similar columns基于现有特征创建新特征

```{r}
#   1. 基本计算
#可以直接使用 R 的基础算术运算符（如 +, -, *, /）来创建新变量。
DATA$new_var = DATA$var1 + DATA$var2

#   2. 使用 dplyr 的 mutate 函数
#dplyr 包的 mutate 函数非常适合在数据框中添加新列或修改现有列。
library(dplyr)
DATA <- DATA %>% 
  mutate(new_var = var1 / var2)

#   3. 条件语句
#使用 ifelse 函数或 dplyr 的 case_when 函数基于条件创建新变量。
DATA$new_var = ifelse(DATA$var1 > threshold, value_if_true, value_if_false)

DATA <- DATA %>% 
  mutate(new_var = case_when(
  condition1 ~ value1,
  condition2 ~ value2,
  TRUE ~ default_value
))

#   4. 日期和时间变量
#使用 lubridate 包来处理和创建基于日期和时间的派生变量。

library(lubridate)
DATA$year <- year(DATA$date_var)
DATA$month <- month(DATA$date_var)

#   5. 文本处理
#使用 stringr 包处理字符串数据，创建基于文本的派生变量。
library(stringr)
DATA$new_var = str_sub(DATA$text_var, 1, 5)  # 提取字符串前五个字符

#   6. 分类变量编码
#使用 factor 或 dplyr 的 mutate 与 as.factor 来创建或修改分类变量
DATA$new_var = as.factor(DATA$var1)
DATA <- DATA %>% 
  mutate(new_var = as.factor(var1))

#   7. 数值转换和标准化
#使用 scale 函数对数值变量进行标准化。
DATA$new_var = scale(DATA$var1, center = TRUE, scale = TRUE)

#   8. 汇总统计
#使用 dplyr 的 group_by 和 summarize 来创建基于组的派生变量。
DATA_summary <- DATA %>% 
  group_by(group_var) %>% 
  summarize(mean_var = mean(var1, na.rm = TRUE))

#   9. 使用数学和统计函数
#R 提供了大量的数学和统计函数，如 log, exp, mean, median 等。
DATA$log_var = log(DATA$var1)
```

# Regression Modelling 建立模型

## Spatial Baseline Model建立空间基线模型

Establishing a spatial baseline model typically refers to creating a basic regression model that accounts for spatial variability. This model serves as a benchmark for comparison, allowing the evaluation of the GWR model or other spatial models against non-spatial models, like ordinary least squares regression. The spatial baseline model usually includes variables relevant to the study but does not incorporate treatments for spatial variability, thus providing a clear view of the model's performance changes after introducing the spatial dimension.

```{r}
# 在深入调整模型之前，建立一个空间基线模型，以了解空间变量对因变量的基本影响

```

## Training set and Testing set 数据分割

```{r}
#如果适用，将数据分割为训练集和测试集，考虑到空间数据的特殊性。
```

## Model Applying

```{r}


```

# 模型评估与调整

## 交叉验证

```{r}
# 使用K折交叉验证来评估模型性能的稳定性。 性能指标：如均方误差（MSE）、决定系数（R²）等。
```

## 调整超参数

```{r}
#使用网格搜索（Grid Search）或随机搜索（Random Search）来找到最佳参数。
```

## Residuals Analysis 残差分析


In spatial regression models, ensuring that residuals are normally distributed is important because many statistical inferences (like tests for the significance of parameters) are based on the assumption of normality. If residuals are not normally distributed, this can affect the reliability of the model and the validity of the conclusions. Therefore, using a Q-Q plot to examine the normality of residuals in spatial regression models is a crucial step in model diagnostics.

```{r}
#  A Q-Q plot is a common method to check if data follows a normal distribution. In a Q-Q plot of a standard normal distribution, if the data points roughly fall along a straight line, it suggests that the data are approximately normally distributed.
# Identifying Outliers: The Q-Q plot can also help identify outliers in the data. Data points that deviate significantly from the line may indicate outliers.

# Create the dataframe for normal distribution reference
data <- rnorm(100) # using the normal distribution random generated numbers.
df <- data.frame(sample = data)

ggplot(df, aes(sample = sample)) +
  stat_qq() +
  stat_qq_line(col = "red") +
  ggtitle("Q-Q Plot") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles")

```


###
```{r}
#特别在回归模型中，检查残差是否呈现随机分布。 
```

# Conclusion

## Summary

## Research Limitation

## Future Research

-   Geography Detector 地理探测器
-   
