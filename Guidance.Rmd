---
title: "Guidance"
author: "BohaoSu"
date: "2023-12-13"
output: 
  pdf_document:
    latex_engine: xelatex
    citation_package: natbib
    toc: TRUE
    number_sections: TRUE
    toc_depth: 2
geometry: "top=25mm, left=30mm, right=30mm, bottom=25mm, heightrounded"
linkcolor: blue
highlight-style: github
bibliography: "reference.bib"
csl: "ucl-institute-of-education-harvard.csl"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = FALSE,
                      warning = interactive(), 
                      message = interactive(), 
                      error = TRUE)
```

# Initial project scope

```{r Packages_Loading,collapse = TRUE,echo=TRUE,eval = TRUE}
library(broom)
library(car)
library(caret)
library(classInt)
library(corrplot)
library(corrr)
library(crosstalk)
library(DiagrammeR)
library(dplyr)
library(fs)
library(geojsonio)
library(ggplot2)
library(ggmap)
library(here)
library(janitor)
library(lmtest)
library(maptools)
library(mapview)
library(OpenStreetMap)
library(patchwork)
library(plotly)
library(purrr)
library(RColorBrewer)
library(readr)
library(rJava)
library(rgdal)
library(RSQLite)
library(rgeos)
library(sf)
library(sp)
library(spatialreg)
library(spatstat)
library(spdep)
library(stringr)
library(tidyverse)
library(tmap)
library(tmaptools)
```

## Research Question:

-   What are the factors that might lead to xxxxxxxxxxxxx scores in the xxxxxxxx city?

## Hypothesis:

-   Null hypothesis: There is complete spatial randomness. No statistical significance exists in a set of given observations. There is no pattern - i.e. complete spatial randomness - in our data. There is no relationship between exam scores and other observed variables across London.
-   Alternative hypothesis: Our data does exhibit a pattern.

## Methodology:

1.  The first step is always cleaning and pre-processing data, which is the foundation for any kinds of analysis and modelling.

2.  Exploration Data Analysis(histograms and Q-Q plots for statistical information, KDE for spatial distribution, DBSCAN for spatial clustering, etc.) need to be done both for non-spatial and spatial fields. This step would clarify the simple relationship and some features inside the data.

3.  Based on research purpose, the regression model also needs two important prerequisite to guarantee its adaptability and rationality.

  -   The first one is "The xxxxxx's happening does have summarizable and discernible spatial distribution characteristics and spatial patterns." This indicates whether a spatial analysis rather than purely quantitative analysis should be utilized to address the research question. Hence, Spatial patterns analysis like KDE or DBSCAN should be operated to check whether there is random occurrences for the xxxxxxxxxxx or not. If the result is complete random distribution, I'll just do the basic quantitative analysis based on the non-spatial data.
  -   The second one is "Spatial location information does play as a crucial and indispensable variable when building regression models." This means which regression model should be utilized to analysis and predict the xxxxxxx. I suppose spatial autocorrelation methods should be used to examine the adaptability of Tobler's Law.[@tobler_computer_1970] If there is no evidence showing geographical elements does affect the dependant variables distribution, then linear regression model or polynomial regression model should be the options. Otherwise, we should consider spatial information and select spatial regression models like spatial lag model, spatial error model or geographically weighted regression models.

4.  Afterwards, some advanced filtering or merging should be operated based on the ESDA, after which some cleansed columns and features could be extracted from the raw data and regarded as the independent variables for regression model to test the hypothesis. The variables selection process should also take some background context and research purpose into consideration.

5.  Then, modelling part should be emphasized on which model should be selected. Regression Model selection will refer to all above previous analysis and prerequisites. After establishing a baseline model, the focus shifts to evaluating and refining this model. This involves comparing the baseline model's performance against the spatial models, using metrics such as R-squared, AIC, or RMSE for validation and visualization. This process of model selection and refinement is central to achieving reliable and meaningful insights from the spatial analysis.

6.  At the End, all results and features would be generalized and summarized, and a primary research conclusion will be drawn towards the initial question.

## Potential Limitation of data and methods

### Data Limitation

-   Issues with Spatial Scale: The spatial scale of the data (e.g., geographic extent and resolution) can affect the analysis outcomes. Different spatial scales may reveal different patterns and relationships.

-   Data Quality and Completeness: The dataset may have missing or inaccurate data, leading to biased analysis results. And we will also drop part of the data due to some NAs or administrative boundaries, which will also impact on dataset's completeness.

### Methods Limitation

-   Non-independence of Spatial Data: Traditional non-spatial regression models often assume independence between observations, which may not hold true for spatial data. Spatial dependence between neighboring locations can impact the accuracy of the model. 

-   Spatial Autocorrelation in Residuals: In many regression models, spatial autocorrelation in residuals is considered a serious violation. If residuals from the model show spatial clustering, it might indicate that key variables are missing from the model.

## RMD environment configuration

Before the specific illustration and analysis procedure, some environment configuration should be down to guarantee this .RMD file's robust on various platform and devices.

-   Download .bib and .csl file remotely for reference

```{r Download_bib_csl, echo=TRUE, results='markup'}

# download reference.bib remotely from my github
download.file("https://github.com/xxxxxxx.bib", 
              destfile=here::here("reference.bib"))

# download reference.bib remotely from my github
download.file("https://raw.githubusercontent.com/BohaoSuCC/CASA0005BohaoSu/main/ucl-institute-of-education-harvard.csl", 
              destfile=here::here("ucl-institute-of-education-harvard.csl"))

```

-   Create Data Folder for Loading and Saving Data

```{r Directory_Folder}
# create the folder storing data for a better robust
folder_name <- "Data"

# get the root dir
root_dir <- here::here()

# construct the full path
folder_path <- file.path(root_dir, folder_name)

# check if the folder already exists
if (!dir.exists(folder_path)) {
  dir.create(folder_path)
  message("Folder '", folder_name, "' created at ", folder_path)
} else {
  message("Folder '", folder_name, "' already exists at ", folder_path)
}

```

-   Define Functions

```{r DefineFunction_Normalization}
# define a function to standardize dataframe
normalize_data <- function(df, columns, method = "standardize") {
  # 确保提供的列存在于DataFrame中
  if (!all(columns %in% names(df))) {
    stop("Some columns not found in the dataframe")
  }

  # 对每个指定的列进行处理
  for (col in columns) {
    # 确保列是数值类型
    if (!is.numeric(df[[col]])) {
      stop("Non-numeric column detected: ", col)
    }

    # 标准化处理
    if (method == "standardize") {
      df[[col]] <- (df[[col]] - mean(df[[col]], na.rm = TRUE)) / sd(df[[col]], na.rm = TRUE)
    }
    # 归一化处理
    else if (method == "normalize") {
      df[[col]] <- (df[[col]] - min(df[[col]], na.rm = TRUE)) / (max(df[[col]], na.rm = TRUE) - min(df[[col]], na.rm = TRUE))
    }
    else {
      stop("Invalid method. Choose 'standardize' or 'normalize'.")
    }
  }
  return(df)
}
```
# Data Introduction

## Downloading, Unzipping and loading the data

```{r Dataset_DownLoading, warning=FALSE, message=FALSE}

#Downloading the relating files and save and unzip it.
download.file("https://data.london.gov.uk/download/statistical-gis-boundary-files-london/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip", 
              destfile=here::here("Data","statistical-gis-boundaries-london.zip"))

```

```{r Dataset_Unzipping, warning=FALSE, message=FALSE}

listfiles<-dir_info(here::here("Data")) %>%
  dplyr::filter(str_detect(path, "london.zip")) %>%
  dplyr::select(path)%>%
  pull()%>%
  #print out the .gz file
  print()%>%
  as.character()%>%
  utils::unzip(exdir=here::here("Data"))

# reading the shp
Londonwards<-fs::dir_info(here::here("Data", 
                                 "statistical-gis-boundaries-london", 
                                 "ESRI"))%>%
  #$ means exact match
  dplyr::filter(str_detect(path, 
                           "London_Ward_CityMerged.shp$"))%>%
  dplyr::select(path)%>%
  dplyr::pull()%>%
  #read in the file in
  sf::st_read()

```

```{r Dataset_Reading, warning=FALSE, message=FALSE, eval = TRUE}

#Reading the csv and Add na argument to make sure csv's robust
# replace all the nas as " "
data_test <- read_csv(here::here("Data","Evictions_20231212.csv"), na=c(" ")) 

```

```{r}

LondonWardProfiles <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/772d2d64-e8c6-46cb-86f9-e52b4c7851bc/ward-profiles-excel-version.csv", 
                               col_names = TRUE, 
                               locale = locale(encoding = 'UTF-8'))


#Reading the shp file
community_areas <- st_read(here::here("Data","geo_export_7fdf694c-62dd-4de4-8f17-0b5ca2408993.shp"))
```

## Data Description

-   The dataset is mainly about xxxxxxxxxxxx, containing xxxxxxxxxxxxxxx in New York city. It is collected by xxxxxx and xxxx through xxxxx and published in the [xxxx's website](https://www.openai.com/).
-   Another data is xxxxx.shp, containing geographical information features about xxxxxxxx in xxxx city, which is published by xxxxx and can be public accessed through [xxxx's website](https://www.openai.com/).

## NA values

In the dataset, the NA values could probably mean the missed data, unrecorded observations, inapplicable data points, etc.

```{r check_NA}
#check all of the columns have been read in correctly
Column_type_list <- evictions_points %>% 
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")
total_rows <- nrow(evictions_points)

#get the na values proportion of each column
Column_NA_ratio_list <- evictions_points %>% 
  summarise_all(~sum(is.na(.))/total_rows) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_NA_ratio")

# check the CRS and any error within spatial data
st_geometry(BoroughMap)

Column_type_list
Column_NA_count_list
```

From the statistical chart we could see there are totally xxxx rows(observations) containing NAs values. Technically, I don't think it is a high rate and these NA values could have a significant impact on my analysis.

Also, I am going to consider how to deal with those NA values with different solutions according to each column's role during my analysis. Anyway, the specific solutions to these NA values should align with research question and analysis requirement, so the detailed Data processing would be demonstrated in Data Cleaning and Processing.


## Accuracy and Biasing

-   Due to the absence of some accuracy information such as measurement errors, data validation processes, etc, I will focus on the biases of the data. According to the description on the website [xxxx's website](https://www.openai.com/), the purpose of collecting these data is mainly to xxxxxxxxxxxxxxxxxxxx, which might bring about the biases of not xxxxxxxxxxxxxxxxxxxxxxxxx. However, I do not think this kind of biases will bring obvious and significant impact on analysis results and conclusions, even though the data collection methods do have limitation which I would elaborate detailedly afterwards.

## Coordinate Reference System (CRS)

-   Explain the coordinate reference system used in the data, including its type (such as geographic or projected coordinate system) and specific name (like WGS 84, UTM, etc.).

```{r}
# transform the non-spatial data into spatial data based on columns'Longitude''Latitude'
Airbnb <- read_csv(here::here("listings.csv")) %>%
  st_as_sf(., coords = c("longitude", "latitude"), 
                   crs = 4326) %>%
    st_transform(., 27700)%>%
    # After, do some relavant filter for the useful info
    filter(room_type == 'Entire home/apt' & availability_365 =='365')

# Transform the CRS
sf_DATA_transformed <- st_transform(sf_DATA, crs = 32650)
```

-   In this analysis, we have selected the [specify CRS, e.g., WGS 84, EPSG:4326] as our Coordinate Reference System (CRS). This CRS aligns well with our study's geographic scope that includes [mention the geographical extent, e.g., multiple countries, global analysis, etc.].

-   Moreover, the impact of using [specify CRS] on my spatial analysis, especially in GWR where a spatial weight matrix really matters, is significant. And that requires distance measurement should be calculated, demonstrated and visualized precisely. Using projected CRS, I believe, should be a better choice for visualization, especially for some local-scale analysis and maps.

# Data Cleaning and Processing 

## Data Column Format Normalization 数据格式标准化
First of all, I think it is necessary to do some data columns' format normalization, which means to make sure all the columns' names are in the same format as camel case. This could make the data more readable and easier to be processed.
```{r}
#use Janitor to clean the column's names
A <- clean_names(A, case_type="camel")
#str_trim(): 去除文本两侧的空格（stringr 包）。
#参数：string（输入文本），side（修剪的方向）。
#默认参数：side = "both"。
DATA$column <- str_trim(DATA$column)

```

## Dealing with NAs in spatial and non-spatial dataset

Some columns, such as xxxxxxxxxx, are extremely important that we couldn't extract any useful information if there are NA values. Besides, its high accuracy makes it harder to fill missing values, which leads us to nothing else but to drop them. Some of the columns, such like xxxx and some categorical data, we also could classify all the NA values as a new category. Some columns like xxxxxxxxxx, we could assume, based on the context of the study and common sense, that the missing values are 0. Although this approach may introduce some degree of inaccuracy, it is considered a practical solution since the proportion of NA values in these columns is very high. Therefore, dropping these columns outright would be an unwise decision.

```{r Deal_NA_nonspatial, results='markup'}
# na.omit(): 从数据中删除含有 NA 的行。
# 参数：object（数据对象）。
# 默认参数：无默认参数。
library(dplyr)
DATA_cleaned <- na.omit(DATA)

# dplyr::filter(): 删除特定条件下的行。
# 参数：.DATA（数据框），...（条件表达式）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  dplyr::filter(!is.na(COLUMN_name))

# tidyr::replace_na(): 用特定值替换 NA。
# 参数：DATA（数据框），replace（用于替换的值）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  replace_na(list(COLUMN = replacement_value))

```

```{r Deal_NA_spatial}
# na.omit(): 从数据中删除含有 NA 的行。
# 参数：object（数据对象）。
# 默认参数：无默认参数。
library(dplyr)
DATA_cleaned <- na.omit(DATA)

# dplyr::filter(): 删除特定条件下的行。
# 参数：.DATA（数据框），...（条件表达式）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  dplyr::filter(!is.na(COLUMN_name))

# tidyr::replace_na(): 用特定值替换 NA。
# 参数：DATA（数据框），replace（用于替换的值）。
# 默认参数：无默认参数。
DATA_cleaned <- DATA %>% 
  replace_na(list(COLUMN = replacement_value))

```

## Converting Datatype

And we will also convert some columns into specific datatype for more covenient processing.

```{r}
# as.numeric(): 将数据转换为数值类型。
# 参数：x（输入数据）。
# 默认参数：无默认参数。
DATA$COLUMN <- as.numeric(DATA$COLUMN)

#as.character(): 将数据转换为字符类型。
#参数：x（输入数据）。
#默认参数：无默认参数。
DATA$COLUMN <- as.character(DATA$COLUMN)

#as.Date(): 将数据转换为日期类型。
#参数：x（输入数据），format（日期格式）。
#默认参数：format = "%Y-%m-%d"。
DATA$date_COLUMN <- as.Date(DATA$date_COLUMN, format = "%Y-%m-%d")

DATA <- DATA %>%
  mutate(COLUMN = str_replace_all(COLUMN, "\\$", "")) %>% # remove dollar sign
  mutate(COLUMN = str_replace_all(COLUMN, ",", "")) %>%  # remove the comma
  mutate(COLUMN = as.numeric(COLUMN))  

```

## Delete or Filter outliers

```{r}
#select all spatial feature with the city boundary and transorm its CRS
BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))%>%
  st_transform(., 27700)

#delete specific rows by filter and some arguments

DATA_cleaned <- DATA %>% 
  dplyr::filter(COLUMN >= lower_limit, COLUMN <= upper_limit)

# only remain points which inside the boundary
BluePlaquesSub <- BluePlaques[BoroughMap, , op=st_within]
# to identify points completely within the borough outline, or a variety of other options such as st_overlaps, st_touches, st_contains, st_disjoint

# plot the map to check to see that they've been removed
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")

```

## Dealing with Repetitive or Unique rows

```{r}

# make sure there is no more repetitive rows in the data
BluePlaques <- distinct(BluePlaques)

#duplicated(): 检测重复行。
#参数：x（输入数据）。
#默认参数：无默认参数。
DATA_cleaned <- DATA[!duplicated(DATA), ]

#unique(): 获取唯一行。
#参数：x（输入数据）。
#默认参数：无默认参数。
DATA_cleaned <- unique(DATA)

```

## DATA Integration 数据整合

```{r}

# 两个non-spatial数据之间，还有inner_join,right_join,full_join
DATA_combined <- left_join(DATA1, DATA2, by = c('SAME_COLUMN_NAME'='SAME_COLUMN_NAME'))

# spatial data join。 Argument could be 
result <- st_join(x, y, op = st_intersects)

#plot the Pointsdata in the area
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(Pointsdata) +
  tm_dots(col = "blue")

```


```{r Set_Window}
# select by attribute
studyarea_window <- BoroughMap %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))

#Check to see that the correct borough has been pulled out
tm_shape(studyarea_window) +
  tm_polygons(col = NA, alpha = 0.5)

#clip the data to our single borough
Pointsdata <- Pointsdata[studyarea_window,]
#check that it's worked
tmap_mode("plot")

tm_shape(studyarea_window) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(Pointsdata) +
  tm_dots(col = "blue")

```

```{r Create_ppp_object}
# select by attribute
studyarea_window <- BoroughMap %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))

#Check to see that the correct borough has been pulled out
tm_shape(studyarea_window) +
  tm_polygons(col = NA, alpha = 0.5)

#create a sp object
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')

# create the window from above studyarea_window
window <- as.owin(studyarea_window)
plot(window)

#create a ppp object
Pointsdata.ppp <- ppp(x=BluePlaquesSub$Longitude,
                          y=BluePlaquesSub$Latitude,
                          window=window)

```


# Exploration Spatial Data Analysis (ESDA)

Exploration Spatial Data Analysis (ESDA) plays an significant role in spatial regression modeling, primarily focusing on three key aspects. 
-   First part contains acquiring statistical characteristics and distribution patterns of all non-spatial data, providing a solid foundation for selecting relevant independent variables for the model. This step is crucial for understanding the underlying structure and relationships within the data. 
-   Second part involves cluster analysis to ascertain if the data exhibits spatial clustering, indicating non-random distribution across the space. This analysis verifies one of the prerequisites for spatial regression modeling, ensuring the data's suitability for such analysis. 

-   Another essential prerequisite will be checked in part3, which is the impact of geographical spatial differences on certain dependent variables in spatial data. This is accomplished by conducting spatial autocorrelation analysis, which helps to confirm if spatial factors significantly influence the variables in question, thereby validating the use of spatial regression techniques.

## Distribution and coorelationship
```{r, echo=TRUE,eval=TRUE, warning=FALSE}
data_test$Latitude <- as.numeric(data_test$Latitude)

clean_data <- data_test %>%
  filter_all(all_vars(!is.na(.)))
```

```{r,}
ggplot(clean_data, aes(x = Latitude)) + 
  geom_histogram(binwidth = 0.01, fill = "blue", color = "black", alpha=0.5)

```

```{r, results='hold'}
# 绘制var1的直方图
ggplot(data_test, aes(x = Latitude)) + 
  geom_histogram(binwidth = 1, fill = "blue", color = "black")

# 绘制var1和var2之间的散点图
ggplot(data, aes(x = var1, y = var2)) + 
  geom_point()

# 箱子图-假设你的数据框架是data，列名为num_var
ggplot(data, aes(y = num_var)) + 
    geom_boxplot(fill = "lightblue", color = "blue")

# 条形图（Bar charts）条形图用于显示分类变量的频率。
ggplot(data, aes(x = cat_var)) + 
    geom_bar(fill = "lightgreen", color = "darkgreen")

# 计算cormatrix----假设你的数据框架是data，选择其中的几个数值型列
numeric_data <- data[c("num_var1", "num_var2", "num_var3")]
# 计算相关系数矩阵
cor_matrix <- cor(numeric_data)
# 绘制相关系数矩阵
corrplot(cor_matrix, method = "circle")
#这段代码首先计算了numeric_data中数值型变量的相关系数矩阵，
#然后使用corrplot()函数绘制出相关系数矩阵。
#method = "circle"表示使用圆圈的方式来表示相关系数的大小和方向。
#"circle"：使用圆圈，圆圈越大，相关性越强；颜色通常用来表示正负相关。
#"square"：与 "circle" 类似，但使用正方形来表示相关系数。
#"ellipse"：使用椭圆形状，椭圆的形状和方向表示相关性的强度和方向。
#"number"：直接在每个格子中显示相关系数的数值。
#"shade"：通过阴影深浅来表示相关系数的大小，通常不显示相关系数的具体数值。
#"color"：仅通过颜色来表示相关系数的大小和方向，类似于热图。
#"pie"：使用饼图来表示相关系数，饼图的大小和填充比例反映相关性的强度和方向。
```

## Several Histograms 多个图合并
```{r, echo=TRUE,eval=TRUE, warning=FALSE}
names(data_test)
```

```{r, echo=TRUE,eval=TRUE, warning=FALSE}

columns_name_datatest <- names(data_test)[c(13,14,15,16)]

# create a new list to save every histogram
plots_list <- list()

#ggplot(clean_data, aes(x = Latitude)) + 
  #geom_histogram(binwidth = 0.05, fill = "blue", color = "black", alpha=0.5)

# create a histogram towards every column and add it into the plot list
for (i in 1:4) {
  column_name <- columns_name_datatest[i]
  pic <- ggplot(data_test, aes(x = !!sym(column_name))) + 
    geom_histogram(fill = "lightblue", color = "black") +
    ggtitle(paste("Histogram of", column_name)) +   #每个图的图名
    xlab(paste("Values of", column_name)) +  # 设置X轴标题
    ylab("Frequency") +  # 设置Y轴标题为“频率”
    theme(
      plot.title = element_text(size = 10, hjust=0.5),  # 设置图表标题字体大小
      axis.title.x = element_text(size = 8),  # 设置X轴标题字体大小
      axis.title.y = element_text(size = 8)   # 设置Y轴标题字体大小
    )
  plots_list[[i]] <- pic
}

# using patchwork to combine every histograms into a 2*2 grids
combined_plot <- wrap_plots(plots_list, ncol = 2) & 
  theme(plot.title = element_text(size = 10, hjust=0.5),
    axis.title.x = element_text(size = 8),
    axis.title.y = element_text(size = 8)
  )

combined_plot
```

## Spatial Distribution 查看空间分布

In order to understand the spatial distribution, I utilized Kernel Density Estimation (KDE) for its analysis. This method could provide with an in-depth examination of xxxxxxxxxxxxxxxxxxxx (specific aspect or feature of the data), enabling a clearer visualization of spatial concentration and patterns within the study area. The KDE plot is particularly effective in highlighting areas of high density or clustering, which is essential for the analysis of xxxxxxxxxxxxxxxxxxxxxx (specific phenomena or geographic feature).
-   From the Heatmaps plot we could see xxxxxxxxxxxxxxxxxxxxxxxxxxx

```{r KDE}
Pointsdata.ppp %>%
  density(., sigma=500) %>%
  plot()
```

-   From the KDE (Kernel Density Estimation) plot, we can observe xxxxxxxxxxxxxxxxx. This suggests that the distribution of data exhibits a trend of xxxxxxxxxxxxxxxxxxxxxxxxxxxxx, which is crucial for understanding xxxxxxxxxxxxxxxxxxxxxxx (specific data characteristic or geographic feature). Particularly in the region of xxxxxxxxxxxxxxxxxxx, this distribution provides key insights into xxxxxxxxxxxxxx (a relevant phenomenon or issue)."

## Spatial Patterns 查看空间模式（如聚集、离散）

In spatial data analysis, identifying whether patterns are clustered or dispersed is crucial, which could also examine the prerequisite about Spatial distribution randomness. Two common methods for this analysis are Ripley's K function and DBSCAN. 

-   Ripley's K function is adopted at quantifying spatial dependence over various scales, offering insights into how spatial processes change with distance. It excels in identifying spatial patterns at specific scales but requires careful handling of distance scales and edge effects. 
-   On the other hand, DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that identifies high-density regions as clusters while marking noise or isolated points. This method is particularly effective for complex or non-homogeneous spatial distributions due to its sensitivity to high-density areas and robustness against noise.While DBSCAN is sensitive to parameter settings, once the appropriate parameters are chosen, it can effectively highlight distinct clustering patterns in a more intuitive and easily interpretable manner. 

Therefore, DBSCAN may be preferred in scenarios involving distinct clusters, irregular distributions, or significant noise in spatial data.

Before proceeding with the DBSCAN clustering analysis, I first utilize the KNNdistplot (k-nearest neighbors distance plot), which is extremely crucial for determining the parameters for the DBSCAN clustering algorithm, especially the eps and minPts parameters. The KNNdistplot could identify the density distribution among data points, thereby xxxxxxxxxxxxx. Based on this plot, I can select an appropriate eps value, which represents the maximum distance for points to be considered as neighbors, and moving on to the DBSCAN for clustering analysis.

```{r}

#create a sp object
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')

#create a ppp object
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)

#first extract the points from the spatial points data frame
PointsdataSub <- Pointsdata %>%
  coordinates(.)%>%
  as.data.frame()

# check and find the proper eps and minpts by using kNNdistplot
BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)
```

After conducting the DBSCAN clustering analysis, the results indicate xxxxxxxxxxxxxxxxxx, revealing spatial clustering patterns in the data. Each cluster represents xxxxxxxxxxxxxxxxxxxxx, while noise points (points not assigned to any cluster) may suggest xxxxxxxxxxxxxxxxxxx. These clusters help us identify xxxxxxxxxxxxxxxxxxxx, such as concentrated trends or anomaly patterns in specific areas. 
```{r}

#now run the DBSCAN analysis
DBSCANoutput <- PointsdataSub %>%
  fpc::dbscan(.,eps = 700, MinPts = 4)

#now plot the results
plot(DBSCANoutput, PointsdataSub, main = "DBSCAN Output", frame = F)
plot(BoroughMap$geometry, add=T)

# add the DBSCAN result back to dataframe
Pointsdata<- PointsdataSub %>%
  mutate(dbcluster=DBSCANoutput$cluster)

#create some convex hull polygons to wrap around the points in our clusters
chulls <- PointsdataSub %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
  hull = factor(hull, chull(coords.x1, coords.x2)))%>%
  arrange(hull)

#drop the cluster =0 out from the dataframe
chulls <- chulls %>%
  filter(dbcluster >=1)
```


```{r}
#create a ggplot2 object from our data
dboutput_plot <- ggplot(data=PointsdataSub, 
                 aes(coords.x1,coords.x2, colour=dbcluster, fill=dbcluster)) 
#add the points in
dboutput_plot <- dboutput_plot + geom_point()
#now the convex hulls
dboutput_plot <- dboutput_plot + geom_polygon(data = chulls, 
                                aes(coords.x1,coords.x2, group=dbcluster), 
                                alpha = 0.5) 
#now plot, setting the coordinates to scale correctly and as a black and white plot 
#(just for the hell of it)...
dbplot + theme_bw() + coord_equal()

```


```{r OSMbasemap}

###add a basemap
##First get the bbox in lat long for Harrow
HarrowWGSbb <- Harrow %>%
  st_transform(., 4326)%>%
  st_bbox()

library(OpenStreetMap)
# create  a basemap
basemap <- OpenStreetMap::openmap(c(51.5549876,-0.4040502),c(51.6405356,-0.2671315),
                         zoom=NULL,
                         "osm")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

#autoplot(basemap_bng) sometimes works
autoplot.OpenStreetMap(basemap_bng)+ 
  geom_point(data=BluePlaquesSubPoints, 
             aes(coords.x1,coords.x2, 
                 colour=dbcluster, 
                 fill=dbcluster)) + 
  geom_polygon(data = chulls, 
               aes(coords.x1,coords.x2, 
                   group=dbcluster,
                   fill=dbcluster), 
               alpha = 0.5)  
```

Additionally, the characteristics and locations of these clusters can be used for xxxxxxxxxxxxxxxxxxx, providing crucial insights for a deeper understanding of potential geographic phenomena in the study area.


## Spatial Autocorrelation 空间自相关性

-   Until now, I have checked the first prerequisite and prove that Data points' distribution do have certain patterns according to geographical information.
-   The next step is to examine do data points' some columns(values) also have their special coorelation with geographical features?

The spatial autocorrelation analysis indicates whether the spatial distribution of variables is random or exhibits spatial dependency. This is crucial for xxxxxxxxxx, as it helps in ensuring the accuracy and validity of the spatial regression model.

### Which method for spatial autocorrelation?

There are several spatial autocorrelation methods such as Global Moran's I[@moran_notes_1950], Local Moran's I[@anselin_local_1995], Geary's C[@geary_contiguity_1954], and Getis-Ord[@ord_local_1995]. And I believe most suitable method depends on the comparing process between research objectives and methods' principles, advantages/disadvantages and applicability. The local Moran's I and Getis-Ord are usually more adaptable to some local analysis, easier to identify local hotspots, coldspots, or spatial anomalies[@abdulhafedh_novel_2017]. 

While my research goal at this step contains two aspects, one is to examine the existence of spatial autocorrelation in a global view, plus that Geary's C focuses more on measuring similarity between values in neighboring areas. Therefore, I choose Global Moran's I for the spaital autocorrelation analysis.

However, Local Moran's I would also be helpful to determine which model to use. Due to the research question's context ane ESDA, it seems that xxxxxxxx has the local variability or spatial heterogeneity in spatial data. So Local Moran's I would also be taken into consideration to see if GWR is more suitable than other models.




### Weight Matrix

Before performing spatial autocorrelation regression analysis, constructing a weight matrix is a prerequisite. The weight matrix, which represents xxxxxxx, allows us to xxxxxxxx. From this matrix, we can infer xxxxxxxxx, which is instrumental in understanding the spatial relationships among the observations."

```{r ManipulationForWeightMatrix}

# 点放到边界里
example<-st_intersects(BoroughMap, PointsdataSub)



# length of each list per polygon and add this as new column
points_sf_joined <- LondonWardsMerged%>%
  mutate(n = lengths(st_intersects(., BluePlaquesSub)))%>%
  janitor::clean_names()%>%
  #calculate area
  mutate(area=st_area(.))%>%
  #then density of the points per ward
  mutate(density=n/area)%>%
  #select density and some other variables 
  dplyr::select(density, ward_name, gss_code, n, average_gcse_capped_point_scores_2014)


points_sf_joined<- points_sf_joined %>%                    
  group_by(gss_code) %>%         
  summarise(density = first(density),
          wardname= first(ward_name),
          plaquecount= first(n))

# plot the map
tm_shape(points_sf_joined) +
    tm_polygons("density",
        style="jenks",
        palette="PuOr",
        midpoint=NA,
        popup.vars=c("wardname", "density"),
        title="Blue Plaque Density")


```

```{r Weight Matrix}
#First calculate the centroids of all Wards in London

coordsW <- points_sf_joined%>%
  st_centroid()%>%
  st_geometry()
  
plot(coordsW,axes=TRUE)


#create a neighbours list
LWard_nb <- points_sf_joined %>%
  poly2nb(., queen=T)

#plot them
plot(LWard_nb, st_geometry(coordsW), col="red")
#add a map underneath
plot(points_sf_joined$geometry, add=T)

#Moran's I requires the spatial weight list type object as opposed to matrix
Lward.lw <- LWard_nb %>%
  nb2listw(., style="C")
```

### Autocorrelation
Now we have defined our matrix, we can calculate the Moran’s I and other associated statistics.

```{r SpatialAutocorrelation}
I_LWard_Global_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  moran.test(., Lward.lw)

I_LWard_Global_Density
```

The Moran's I statistic is xxxxxxxxx. which means xxxxxxxxxxxxxxxxxxx. These results are significant for xxxxxxxxxxxx, as they provide insights into the spatial dependence and help in xxxxxxxxxxxx. Additionally, this could be very important for selecting the proper regression model, about whether we should take the spatial features into consideration in regression model.


# Variables Selection 特征选择

## Selecting Independent Variables based on ESDA and Research Question

-   The first step is to select independent variables based on ESDA and research question. 

Considering that there is no enough time do the all the ESDA for all the variables, I will select some variables mainly based on the simple correlation linearship analysis and some mechanism explanation towards the question.

-   Create Variables for Generalizing several similar columns基于现有特征创建新特征

Besides, there are also some variables that are similar to each other, and I will create new variables to generalize them, which would also be helpful for preventing the potential multicollinearity problem.

```{r Creating_var}
#   1. 基本计算
#可以直接使用 R 的基础算术运算符（如 +, -, *, /）来创建新变量。
DATA$new_var = DATA$var1 + DATA$var2

#   2. 使用 dplyr 的 mutate 函数
#dplyr 包的 mutate 函数非常适合在数据框中添加新列或修改现有列。
library(dplyr)
DATA <- DATA %>% 
  mutate(new_var = var1 / var2)

#   3. 条件语句
#使用 ifelse 函数或 dplyr 的 case_when 函数基于条件创建新变量。
DATA$new_var = ifelse(DATA$var1 > threshold, value_if_true, value_if_false)

DATA <- DATA %>% 
  mutate(new_var = case_when(
  condition1 ~ value1,
  condition2 ~ value2,
  TRUE ~ default_value
))

#   4. 日期和时间变量
#使用 lubridate 包来处理和创建基于日期和时间的派生变量。

library(lubridate)
DATA$year <- year(DATA$date_var)
DATA$month <- month(DATA$date_var)

#   5. 文本处理
#使用 stringr 包处理字符串数据，创建基于文本的派生变量。
library(stringr)
DATA$new_var = str_sub(DATA$text_var, 1, 5)  # 提取字符串前五个字符

#   6. 分类变量编码
#使用 factor 或 dplyr 的 mutate 与 as.factor 来创建或修改分类变量
DATA$new_var = as.factor(DATA$var1)
DATA <- DATA %>% 
  mutate(new_var = as.factor(var1))

#   7. 数值转换和标准化
#使用 scale 函数对数值变量进行标准化。
DATA$new_var = scale(DATA$var1, center = TRUE, scale = TRUE)

#   8. 汇总统计
#使用 dplyr 的 group_by 和 summarize 来创建基于组的派生变量。
DATA_summary <- DATA %>% 
  group_by(group_var) %>% 
  summarize(mean_var = mean(var1, na.rm = TRUE))

#   9. 使用数学和统计函数
#R 提供了大量的数学和统计函数，如 log, exp, mean, median 等。
DATA$log_var = log(DATA$var1)
```



## DATA Normalization and Standardlization 数据规范化与标准化
-   which variables should be transformed? Here I use the Tukey’s ladder of transformations to decide which variables should be transformed. I must admit that I don't have the insightful understanding of transforming variables, and I just follow the rules of thumb and do the simple transformations as the following codes.

```{r Tukey’s ladder of transformations}
#Tukey’s ladder of transformations
#
symbox(~median_house_price_2014,    
       LonWardProfiles,            
       na.rm=T,                   
       powers=seq(-3,3,by=.5))     

```

```{r StandardizlationFromR}

# 数据规范化
DATA_normalized <- as.DATA.frame(lapply(DATA, normalize))

#   2. 标准化（Standardization）
#标准化指的是将数据转换为均值为0，标准差为1的分布。这通过从每个观测中减去均值并除以标准差来实现
DATA_standardized <- scale(DATA)
#scale(x, center = TRUE, scale = TRUE)
#x 是要进行标准化的数据。
#center = TRUE 表示数据会先减去它的均值。
#scale = TRUE 表示数据会除以它的标准差。

#caret 包提供了更多高级的预处理功能。
#使用 preProcess 函数进行规范化。

preProcValues <- preProcess(DATA, method = c("range"))
preProcValues <- preProcess(DATA, method = c("center", "scale"))
DATA_normalized <- predict(preProcValues, DATA)
#preProcess(x, method)
#x 是要处理的数据。
#method = c("range") 指定使用范围规范化。
#method = c("center", "scale") 指定使用数据中心化和标准化。

DATA_normalized <- DATA %>%
  mutate_if(is.numeric, normalize)

DATA_standardized <- DATA %>%
  mutate_if(is.numeric, scale)

```

# Regression Modelling 建立模型

## Model Selection 模型选择

Based on previous exploratory data analysis, I will use the following variables to build the regression models.
"xxxxxxxxx", "xxxxxxxxxx", "xxxxxxxx", "xxxxxxx", "xxxxxxxxxxxxx"

Also, 

```{r SpatialLaggedRegression}

#Original Spatial Lagged regression Model
model2 <- lm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + 
               log(median_house_price_2014), data = LonWardProfiles)

tidy(model2)

#queen’s case weights matrix
slag_dv_model2_queen <- lagsarlm(average_gcse_capped_point_scores_2014 ~ unauthorised_absence_in_all_schools_percent_2013 + 
               log(median_house_price_2014), 
               data = LonWardProfiles, 
               nb2listw(LWard_nb, style="C"), 
               method = "eigen")

#what do the outputs show?
tidy(slag_dv_model2_queen)
```



```{r}
#glance() gives model stats but this need something produced from a linear model
#here we have used lagsarlm()
glance(slag_dv_model2_queen)
```

## 模型评估与调整

### Residuals Analysis 残差分析

In spatial regression models, ensuring that residuals are normally distributed is important because many statistical inferences (like tests for the significance of parameters) are based on the assumption of normality. If residuals are not normally distributed, this can affect the reliability of the model and the validity of the conclusions. Therefore, using a Q-Q plot to examine the normality of residuals in spatial regression models is a crucial step in model diagnostics.

```{r}
#  A Q-Q plot is a common method to check if data follows a normal distribution. In a Q-Q plot of a standard normal distribution, if the data points roughly fall along a straight line, it suggests that the data are approximately normally distributed.
# Identifying Outliers: The Q-Q plot can also help identify outliers in the data. Data points that deviate significantly from the line may indicate outliers.

# Create the dataframe for normal distribution reference
data <- rnorm(100) # using the normal distribution random generated numbers.
df <- data.frame(sample = data)

ggplot(df, aes(sample = sample)) +
  stat_qq() +
  stat_qq_line(col = "red") +
  ggtitle("Q-Q Plot") +
  xlab("Theoretical Quantiles") +
  ylab("Sample Quantiles")

```


###
```{r}
#特别在回归模型中，检查残差是否呈现随机分布。 
```

# Conclusion



## Summary



## Research Limitation


```{r}
#使用网格搜索（Grid Search）或随机搜索（Random Search）来找到最佳参数。
```


## Future Research

-   Future steps for model optimization. 交叉验证调整超参数
